{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Abdul Raheman (MSCS-634-B01) ProjectDeliverable_3"
      ],
      "metadata": {
        "id": "rG9KsYlPQF1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSg1B1G4QEju",
        "outputId": "7228487b-efb8-4f59-8a55-fc075ba8dec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data prepared successfully\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Data prepared successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train_scaled, y_train)\n",
        "y_pred_dt = dt.predict(X_test_scaled)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "y_prob_dt = dt.predict_proba(X_test_scaled)[:, 1]\n",
        "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)\n",
        "auc_dt = auc(fpr_dt, tpr_dt)\n",
        "\n",
        "print(f\"DT Accuracy: {acc_dt:.4f}\")\n",
        "print(f\"DT F1 Score: {f1_dt:.4f}\")\n",
        "print(\"DT Confusion Matrix:\\n\", cm_dt)\n",
        "print(f\"DT AUC: {auc_dt:.4f}\")\n",
        "\n",
        "# SVM\n",
        "svc = SVC(probability=True, random_state=42)\n",
        "svc.fit(X_train_scaled, y_train)\n",
        "y_pred_svc = svc.predict(X_test_scaled)\n",
        "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
        "f1_svc = f1_score(y_test, y_pred_svc)\n",
        "cm_svc = confusion_matrix(y_test, y_pred_svc)\n",
        "y_prob_svc = svc.predict_proba(X_test_scaled)[:, 1]\n",
        "fpr_svc, tpr_svc, _ = roc_curve(y_test, y_prob_svc)\n",
        "auc_svc = auc(fpr_svc, tpr_svc)\n",
        "\n",
        "print(f\"SVC Accuracy: {acc_svc:.4f}\")\n",
        "print(f\"SVC F1 Score: {f1_svc:.4f}\")\n",
        "print(\"SVC Confusion Matrix:\\n\", cm_svc)\n",
        "print(f\"SVC AUC: {auc_svc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5vNvNX8QdUa",
        "outputId": "70c8412f-0769-4067-8d58-5609b32be339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT Accuracy: 0.9474\n",
            "DT F1 Score: 0.9577\n",
            "DT Confusion Matrix:\n",
            " [[40  3]\n",
            " [ 3 68]]\n",
            "DT AUC: 0.9440\n",
            "SVC Accuracy: 0.9825\n",
            "SVC F1 Score: 0.9861\n",
            "SVC Confusion Matrix:\n",
            " [[41  2]\n",
            " [ 0 71]]\n",
            "SVC AUC: 0.9974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
        "grid = GridSearchCV(SVC(probability=True, random_state=42), param_grid, cv=5, scoring='f1')\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_svc = grid.best_estimator_\n",
        "y_pred_best = best_svc.predict(X_test_scaled)\n",
        "acc_best = accuracy_score(y_test, y_pred_best)\n",
        "f1_best = f1_score(y_test, y_pred_best)\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "y_prob_best = best_svc.predict_proba(X_test_scaled)[:, 1]\n",
        "fpr_best, tpr_best, _ = roc_curve(y_test, y_prob_best)\n",
        "auc_best = auc(fpr_best, tpr_best)\n",
        "\n",
        "print(f\"Best Params: {grid.best_params_}\")\n",
        "print(f\"Tuned SVC Accuracy: {acc_best:.4f}\")\n",
        "print(f\"Tuned SVC F1 Score: {f1_best:.4f}\")\n",
        "print(\"Tuned SVC Confusion Matrix:\\n\", cm_best)\n",
        "print(f\"Tuned SVC AUC: {auc_best:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruNpI4flQ9PI",
        "outputId": "d7ea2097-56b5-41ed-937e-92d12f881c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Tuned SVC Accuracy: 0.9825\n",
            "Tuned SVC F1 Score: 0.9861\n",
            "Tuned SVC Confusion Matrix:\n",
            " [[41  2]\n",
            " [ 0 71]]\n",
            "Tuned SVC AUC: 0.9974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "\n",
        "# KMeans with 2 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Metrics\n",
        "sil_score = silhouette_score(X_scaled, cluster_labels)\n",
        "ari = adjusted_rand_score(y, cluster_labels)\n",
        "\n",
        "print(f\"KMeans Silhouette Score: {sil_score:.4f}\")\n",
        "print(f\"KMeans ARI: {ari:.4f}\")\n",
        "\n",
        "# PCA for visualization description\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "centers_pca = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "print(\"Cluster centers in PCA space:\\n\", centers_pca)\n",
        "\n",
        "# Cluster distribution\n",
        "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
        "print(\"Cluster sizes:\", dict(zip(unique, counts)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDvFeKh9RD0M",
        "outputId": "f50d0a3b-fef8-4aac-9d49-182e6cb6b86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans Silhouette Score: 0.3447\n",
            "KMeans ARI: 0.6765\n",
            "Cluster centers in PCA space:\n",
            " [[ 4.40952193 -0.07888523]\n",
            " [-2.1758271   0.03892499]]\n",
            "Cluster sizes: {np.int32(0): np.int64(188), np.int32(1): np.int64(381)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top 5 features correlated with target (absolute)\n",
        "corr = X.corrwith(y).abs().sort_values(ascending=False)[:6]  # Extra to avoid index issues\n",
        "top_features = corr.index.tolist()\n",
        "print(\"Top features:\", top_features)\n",
        "\n",
        "# Bin into high/low based on median\n",
        "df = X.copy()\n",
        "df['target'] = y\n",
        "for feat in top_features:\n",
        "    median = df[feat].median()\n",
        "    df[feat + '_high'] = (df[feat] > median).astype(int)\n",
        "\n",
        "# Bin target: malignant if target==0\n",
        "df['malignant'] = (y == 0).astype(int)  # 1 if malignant\n",
        "\n",
        "# Transactional columns: high feats and malignant\n",
        "trans_cols = [f + '_high' for f in top_features] + ['malignant']\n",
        "transactions = df[trans_cols]\n",
        "\n",
        "# Adjust min_support to 0.3 to include malignant (~0.37)\n",
        "min_support = 0.3\n",
        "supports = transactions.mean()\n",
        "freq_single = supports[supports >= min_support]\n",
        "print(\"Frequent singletons:\\n\", freq_single)\n",
        "\n",
        "# For pairs\n",
        "from itertools import combinations\n",
        "pairs = list(combinations(trans_cols, 2))\n",
        "pair_supports = {}\n",
        "for col1, col2 in pairs:\n",
        "    support = (transactions[col1] & transactions[col2]).mean()\n",
        "    if support >= min_support:\n",
        "        pair_supports[(col1, col2)] = support\n",
        "print(\"Frequent pairs:\\n\", pair_supports)\n",
        "\n",
        "# For rules\n",
        "rules = []\n",
        "min_conf = 0.7\n",
        "for (ante, cons), supp in pair_supports.items():\n",
        "    conf = supp / supports[ante]\n",
        "    if conf >= min_conf:\n",
        "        lift = conf / supports[cons]\n",
        "        rules.append(f\"{ante} => {cons}: support={supp:.4f}, conf={conf:.4f}, lift={lift:.4f}\")\n",
        "\n",
        "    conf_rev = supp / supports[cons]\n",
        "    if conf_rev >= min_conf:\n",
        "        lift_rev = conf_rev / supports[ante]\n",
        "        rules.append(f\"{cons} => {ante}: support={supp:.4f}, conf={conf_rev:.4f}, lift={lift_rev:.4f}\")\n",
        "\n",
        "# Filter rules involving 'malignant'\n",
        "malignant_rules = [r for r in rules if 'malignant' in r]\n",
        "print(\"High confidence rules involving malignant:\\n\", malignant_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeNOPnU4Sgwg",
        "outputId": "4c24798e-31c5-4952-fec8-d8b97bb34de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top features: ['worst concave points', 'worst perimeter', 'mean concave points', 'worst radius', 'mean perimeter', 'worst area']\n",
            "Frequent singletons:\n",
            " worst concave points_high    0.499121\n",
            "worst perimeter_high         0.499121\n",
            "mean concave points_high     0.499121\n",
            "worst radius_high            0.497364\n",
            "mean perimeter_high          0.499121\n",
            "worst area_high              0.499121\n",
            "malignant                    0.372583\n",
            "dtype: float64\n",
            "Frequent pairs:\n",
            " {('worst concave points_high', 'worst perimeter_high'): np.float64(0.421792618629174), ('worst concave points_high', 'mean concave points_high'): np.float64(0.45694200351493847), ('worst concave points_high', 'worst radius_high'): np.float64(0.4147627416520211), ('worst concave points_high', 'mean perimeter_high'): np.float64(0.4024604569420035), ('worst concave points_high', 'worst area_high'): np.float64(0.4147627416520211), ('worst concave points_high', 'malignant'): np.float64(0.35676625659050965), ('worst perimeter_high', 'mean concave points_high'): np.float64(0.4182776801405975), ('worst perimeter_high', 'worst radius_high'): np.float64(0.4833040421792619), ('worst perimeter_high', 'mean perimeter_high'): np.float64(0.46572934973637964), ('worst perimeter_high', 'worst area_high'): np.float64(0.48154657293497366), ('worst perimeter_high', 'malignant'): np.float64(0.3602811950790861), ('mean concave points_high', 'worst radius_high'): np.float64(0.4112478031634446), ('mean concave points_high', 'mean perimeter_high'): np.float64(0.40421792618629176), ('mean concave points_high', 'worst area_high'): np.float64(0.4077328646748682), ('mean concave points_high', 'malignant'): np.float64(0.3532513181019332), ('worst radius_high', 'mean perimeter_high'): np.float64(0.4727592267135325), ('worst radius_high', 'worst area_high'): np.float64(0.492091388400703), ('worst radius_high', 'malignant'): np.float64(0.36203866432337434), ('mean perimeter_high', 'worst area_high'): np.float64(0.4710017574692443), ('mean perimeter_high', 'malignant'): np.float64(0.3444639718804921), ('worst area_high', 'malignant'): np.float64(0.36203866432337434)}\n",
            "High confidence rules involving malignant:\n",
            " ['worst concave points_high => malignant: support=0.3568, conf=0.7148, lift=1.9185', 'malignant => worst concave points_high: support=0.3568, conf=0.9575, lift=1.9185', 'worst perimeter_high => malignant: support=0.3603, conf=0.7218, lift=1.9374', 'malignant => worst perimeter_high: support=0.3603, conf=0.9670, lift=1.9374', 'mean concave points_high => malignant: support=0.3533, conf=0.7077, lift=1.8996', 'malignant => mean concave points_high: support=0.3533, conf=0.9481, lift=1.8996', 'worst radius_high => malignant: support=0.3620, conf=0.7279, lift=1.9537', 'malignant => worst radius_high: support=0.3620, conf=0.9717, lift=1.9537', 'malignant => mean perimeter_high: support=0.3445, conf=0.9245, lift=1.8523', 'worst area_high => malignant: support=0.3620, conf=0.7254, lift=1.9468', 'malignant => worst area_high: support=0.3620, conf=0.9717, lift=1.9468']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rules show strong links between high feature values and cancer (for example, \"worst radius_high => malignant\" with conf=0.7279, lift=1.9537â€”almost twice as likely).  It is more likely that reverse rules will work (for example,'malignant => worst radius_high' conf=0.9717), which means that most malignant cases have high severity features.\n",
        "\n",
        "  For example, if a tumor has a high worst perimeter, a biopsy should be done as soon as possible (support=0.3603).  In healthcare AI, rules can make things easier to understand. This helps doctors find risk factors that lead to faster diagnoses and more personalized care. By pointing out high-lift combinations early, rules may also lower mortality."
      ],
      "metadata": {
        "id": "PyOcRUxGStlH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvWxobw5Sjxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}